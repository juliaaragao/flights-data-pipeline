# ETL with Python + SQLite

A simple, clean and fully reproducible ETL data pipeline designed for flight delay analysis.
The goal of this project is to demonstrate good engineering practices for data extraction, cleaning, feature creation and loading into a relational database.

**This project uses:**
- Kaggle Flights Dataset (5.8M rows) - https://www.kaggle.com/datasets/usdot/flight-delays/data?select=flights.csv 
- UV for environment and dependency management
- Python (modular ETL inside src/)
- pandas for data transformation
- SQLite as storage layer

This repository is structured as a small but realistic pipeline that a Data Engineer or Data Analyst could use as the foundation for delay analysis or KPI dashboards.

## Project Structure
flights-data-pipeline/
│
├── data/
│   ├── raw/          <- original Kaggle CSV files (not tracked in Git)
│   └──  cleaned/      <- cleaned dataset generated by the pipeline
│   
│
├── database/
│   └── flights_data.db   ← SQLite database generated by ETL
|
├──notebooks/
|    └── eda.ipynb     ← exploratory data analysis 
│
├── src/
   └── flights_data_pipeline/
       ├── extract.py
       ├── transform.py
       ├── load.py
       ├── config.py
       └── run_pipeline.py


## Instalation
### Clone the repository
git clone https://github.com/<your-username>/flights-data-pipeline.git
cd flights-data-pipeline

### Installing dependencies using UV
uv sync

### Clone the repository
Download kaggle csv files : https://www.kaggle.com/datasets/usdot/flight-delays/data?select=flights.csv 
Place the Kaggle file inside: data/raw/flights.csv

## Running full pipeline
Just one command: uv run python src/flights_data_pipeline/run_pipeline.py

**1. Extract**
- Loads and reads the raw CSV from data/raw/.
**2. Transform**
- Creates FLIGHT_DATE
- Creates IS_DELAYED_15
- Removes duplicates (if it exists)
- Keeps relevant columns
- Saves cleaned_flights_data.csv
**3. Load**
- Writes the cleaned data into SQLite
- Creates the table flights





